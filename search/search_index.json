{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction to AutoML \u00b6 Automated Machine Learning , or AutoML for short, is a novel and expanding field in the intersection of machine learning, optimization, and software engineering. It's purpose is to progresively automate the most common (and often boring) tasks of conventional machine learning workflows. Tasks such as data preprocessing, feature extraction and selection, model selection, hyperparameter tunning, model validation, deployment, and monitoring. Despite its novelty, AutoML has become prominent in the last few years, as more profesionals from every field get into machine learning, often without a solid background in machine learning and no time to learn all the necessary theory. The following is a beginner-friendly introduction to the AutoML field. Our purpose is not to survey all the deep theory behind AutoML, but rather to provide an entry-point for newcommers, interested both in the research aspects and the practical aspects of AutoML. To narrow our focus, we decided to analize a set of practical AutoML systems, and extend outwards into the underlying theory guided by the paradigms and concepts that are most commonly used in these practical scenarios. To consider an AutoML system for inclusion, we defined a set of loose criteria, mostly regarding its status as a software product intended for broad use rather than, say, a reference implementation of a novel technique for research purposes. Thus, we consider a broad range of systems, both open-source and commercial, in different levels of maturity and with widely different features, as long as they fit the above criteria. The list of AutoML systems considered in this survey, and the features that are evaluated, are the result of a colaborative effort . Based on the analized systems we defined a set of conceptual features that help mapping the AutoML field, at least from a practical point-of-view. These features involve both internal and external characteristics of the systems. The internal characteristics refer to how the system works, e.g., the techniques it uses for optimizing and searching machine learning pipelines, and the types of hyperparameters it can represent. The external characteristics refer to the types of tasks that can be solved with the system and the way the user interacts with it. Based on these characteristics, we created a taxonomy of commonly used theoretical concepts, principle, and paradigims in the AutoML field, which guide this survey. We provide follow-up references on many of the topics we cover. AutoML in a nutshell \u00b6 At its core, AutoML is about providing tools to automate the process of designing, training, validating, and deploying a machine learning pipeline for a given problem. Machine learning pipelines come in a variety of flavours, but they are often composed of a set of atomic operators (e.g., a feature selection algorithm, or a specific machine learning model), each of which can be configured by one or more hyperparameters (e.g., the number and type of layers and neurons per layer in a neural network, or the regularization factor in logistic regression). Each operator performs some task, often associated with an input data source, and producing an output that is fed to subsequent operators. The objective is to find a pipeline that is optimal, or close to optimal, in solving a given machine learning problem, among a set of posible pipelines. Thus, we can see an AutoML system, on a broad perspective, as a computational solution that receives a machine learning problem definition of some kind~(e.g., classification, regression, clustering) and associated training data, and it outputs a suitable machine learning pipeline, composed of one or more atomic operators, that is close to optimal in solving that problem according to some predefined performance metric(s). The core of any AutoML system is a machine learning pipeline optimization engine . Three key components can be identified in any such engine: a search space, a search strategy, and a performance estimation function. Search space \u00b6 The search space defines which types of pipelines and operators are available. It is often restricted by an underlying machine learning library (or libraries) that support the actual implementation of said operators. For example, AutoML systems based on scikit-learn can only output pipelines composed of scikit-learn estimators and transformers, while AutoML systems based on keras or pytorch can output pipelines that correspond to neural network architectures implemented in terms of these frameworks' APIs. The search space also defines which hyperparameters are optimized, and to what extent. Search strategy \u00b6 A search strategy is an algorithm to efficiently explore a given search space and find the optimal pipelines. The search space can be arbitrarily big, exponential in size with respect to the number of available operators, and potentially unbounded in some of the hyperparameters (e.g., you can add as many layers as you want to a neural network). In machine learning, the difference between a useless solution and a highly effective one often boils down to the use of one type of algorithm over another, or even to their specific configurations. An effective search strategy must be able to quickly focus on promising regions of the search space, while still ensuring a thorough enough exploration to avoid missing potentially good solutions. This balance between exploration and exploitation is the crux of the problem that all search strategies are designed to deal with. Evaluation method \u00b6 Finally, the evaluation method measures the expected performance of any given pipeline in the task at hand. The simplest evaluation method consists in evaluating the pipeline in a validation dataset, but evaluating machine learning algorithms is a costly task, especially when using models with millions or billions of parameters, like the largest neural networks. Thus, we often want to estimate this performance either by evaluating in smaller sets or by creating surrogate functions that approximate it. Furthermore, we often want to optimize more than one performance indicator, which may be in contradiction, such as accuracy versus model complexity (e.g., to reduce inference time in a production scenario). Flavours of AutoML \u00b6 Warning This section is under construction Over the years several different conceptualizations of AutoML have been proposed, some more flexible than others. We'll review next some of the most common flavours of AutoML from a high-level point of view, without dwelving too deep into their specifities. Afterwards, when we have introduced all the building blocks of the AutoML process, we'll be able to look back at these definitions in more details. Hyperparameter optimization \u00b6 Full model selection \u00b6 Combined algorithm selection and hyperparameter optimization \u00b6 Neural architecture search \u00b6 Heterogenous automated machine learning \u00b6 What's next \u00b6 The three components mentioned above (search space, search strategy, and performance estimation function) make up the core of the \"internal characteristics\" of an AutoML system. In this survey, we are also interested in \"external\" characteristics, such as the types of machine learning tasks that can be solved, the interfaces by which users interact with the system, the steps of the machine learning workflow that are covered, and other software engineering concerns. In the next few sections we'll review the basic theory behind the core AutoML process. We'll introduce the most common types of search spaces and strategies, and some interesting performance estimation functions. Finally, we'll present a comparison of many practical AutoML systems in terms of all of these characteristics, and provide some rules guidelines for choosing an appropiate tool for a given task.","title":"Introduction"},{"location":"#introduction-to-automl","text":"Automated Machine Learning , or AutoML for short, is a novel and expanding field in the intersection of machine learning, optimization, and software engineering. It's purpose is to progresively automate the most common (and often boring) tasks of conventional machine learning workflows. Tasks such as data preprocessing, feature extraction and selection, model selection, hyperparameter tunning, model validation, deployment, and monitoring. Despite its novelty, AutoML has become prominent in the last few years, as more profesionals from every field get into machine learning, often without a solid background in machine learning and no time to learn all the necessary theory. The following is a beginner-friendly introduction to the AutoML field. Our purpose is not to survey all the deep theory behind AutoML, but rather to provide an entry-point for newcommers, interested both in the research aspects and the practical aspects of AutoML. To narrow our focus, we decided to analize a set of practical AutoML systems, and extend outwards into the underlying theory guided by the paradigms and concepts that are most commonly used in these practical scenarios. To consider an AutoML system for inclusion, we defined a set of loose criteria, mostly regarding its status as a software product intended for broad use rather than, say, a reference implementation of a novel technique for research purposes. Thus, we consider a broad range of systems, both open-source and commercial, in different levels of maturity and with widely different features, as long as they fit the above criteria. The list of AutoML systems considered in this survey, and the features that are evaluated, are the result of a colaborative effort . Based on the analized systems we defined a set of conceptual features that help mapping the AutoML field, at least from a practical point-of-view. These features involve both internal and external characteristics of the systems. The internal characteristics refer to how the system works, e.g., the techniques it uses for optimizing and searching machine learning pipelines, and the types of hyperparameters it can represent. The external characteristics refer to the types of tasks that can be solved with the system and the way the user interacts with it. Based on these characteristics, we created a taxonomy of commonly used theoretical concepts, principle, and paradigims in the AutoML field, which guide this survey. We provide follow-up references on many of the topics we cover.","title":"Introduction to AutoML"},{"location":"#automl-in-a-nutshell","text":"At its core, AutoML is about providing tools to automate the process of designing, training, validating, and deploying a machine learning pipeline for a given problem. Machine learning pipelines come in a variety of flavours, but they are often composed of a set of atomic operators (e.g., a feature selection algorithm, or a specific machine learning model), each of which can be configured by one or more hyperparameters (e.g., the number and type of layers and neurons per layer in a neural network, or the regularization factor in logistic regression). Each operator performs some task, often associated with an input data source, and producing an output that is fed to subsequent operators. The objective is to find a pipeline that is optimal, or close to optimal, in solving a given machine learning problem, among a set of posible pipelines. Thus, we can see an AutoML system, on a broad perspective, as a computational solution that receives a machine learning problem definition of some kind~(e.g., classification, regression, clustering) and associated training data, and it outputs a suitable machine learning pipeline, composed of one or more atomic operators, that is close to optimal in solving that problem according to some predefined performance metric(s). The core of any AutoML system is a machine learning pipeline optimization engine . Three key components can be identified in any such engine: a search space, a search strategy, and a performance estimation function.","title":"AutoML in a nutshell"},{"location":"#search-space","text":"The search space defines which types of pipelines and operators are available. It is often restricted by an underlying machine learning library (or libraries) that support the actual implementation of said operators. For example, AutoML systems based on scikit-learn can only output pipelines composed of scikit-learn estimators and transformers, while AutoML systems based on keras or pytorch can output pipelines that correspond to neural network architectures implemented in terms of these frameworks' APIs. The search space also defines which hyperparameters are optimized, and to what extent.","title":"Search space"},{"location":"#search-strategy","text":"A search strategy is an algorithm to efficiently explore a given search space and find the optimal pipelines. The search space can be arbitrarily big, exponential in size with respect to the number of available operators, and potentially unbounded in some of the hyperparameters (e.g., you can add as many layers as you want to a neural network). In machine learning, the difference between a useless solution and a highly effective one often boils down to the use of one type of algorithm over another, or even to their specific configurations. An effective search strategy must be able to quickly focus on promising regions of the search space, while still ensuring a thorough enough exploration to avoid missing potentially good solutions. This balance between exploration and exploitation is the crux of the problem that all search strategies are designed to deal with.","title":"Search strategy"},{"location":"#evaluation-method","text":"Finally, the evaluation method measures the expected performance of any given pipeline in the task at hand. The simplest evaluation method consists in evaluating the pipeline in a validation dataset, but evaluating machine learning algorithms is a costly task, especially when using models with millions or billions of parameters, like the largest neural networks. Thus, we often want to estimate this performance either by evaluating in smaller sets or by creating surrogate functions that approximate it. Furthermore, we often want to optimize more than one performance indicator, which may be in contradiction, such as accuracy versus model complexity (e.g., to reduce inference time in a production scenario).","title":"Evaluation method"},{"location":"#flavours-of-automl","text":"Warning This section is under construction Over the years several different conceptualizations of AutoML have been proposed, some more flexible than others. We'll review next some of the most common flavours of AutoML from a high-level point of view, without dwelving too deep into their specifities. Afterwards, when we have introduced all the building blocks of the AutoML process, we'll be able to look back at these definitions in more details.","title":"Flavours of AutoML"},{"location":"#hyperparameter-optimization","text":"","title":"Hyperparameter optimization"},{"location":"#full-model-selection","text":"","title":"Full model selection"},{"location":"#combined-algorithm-selection-and-hyperparameter-optimization","text":"","title":"Combined algorithm selection and hyperparameter optimization"},{"location":"#neural-architecture-search","text":"","title":"Neural architecture search"},{"location":"#heterogenous-automated-machine-learning","text":"","title":"Heterogenous automated machine learning"},{"location":"#whats-next","text":"The three components mentioned above (search space, search strategy, and performance estimation function) make up the core of the \"internal characteristics\" of an AutoML system. In this survey, we are also interested in \"external\" characteristics, such as the types of machine learning tasks that can be solved, the interfaces by which users interact with the system, the steps of the machine learning workflow that are covered, and other software engineering concerns. In the next few sections we'll review the basic theory behind the core AutoML process. We'll introduce the most common types of search spaces and strategies, and some interesting performance estimation functions. Finally, we'll present a comparison of many practical AutoML systems in terms of all of these characteristics, and provide some rules guidelines for choosing an appropiate tool for a given task.","title":"What's next"},{"location":"bayesian_strategy_examples/","text":"Auto-Pytorch , Auto-Sklearn , Auto-WEKA , AutoKeras , Azure ML , Hyperopt-sklearn , KNIME AutoML , Vertex AI .","title":"Bayesian strategy examples"},{"location":"comparison/","text":"Comparison between popular AutoML systems \u00b6 Warning This section is under construction","title":"Comparison"},{"location":"comparison/#comparison-between-popular-automl-systems","text":"Warning This section is under construction","title":"Comparison between popular AutoML systems"},{"location":"constructive_strategy_examples/","text":"AutoGluon , H2O AutoML .","title":"Constructive strategy examples"},{"location":"evaluation-methods/","text":"Evaluation methods \u00b6 Warning This section is under construction Cross-validation \u00b6 Multi-fidelity \u00b6","title":"Evaluation Methods"},{"location":"evaluation-methods/#evaluation-methods","text":"Warning This section is under construction","title":"Evaluation methods"},{"location":"evaluation-methods/#cross-validation","text":"","title":"Cross-validation"},{"location":"evaluation-methods/#multi-fidelity","text":"","title":"Multi-fidelity"},{"location":"evolutionary_strategy_examples/","text":"AutoGOAL , RECIPE , TPOT .","title":"Evolutionary strategy examples"},{"location":"fixed_pipeline_examples/","text":"Auto-Pytorch , Auto-Sklearn , Auto-WEKA , AutoGOAL , AutoGluon , AutoNLP , Azure ML , H2O AutoML , Hyperopt-sklearn , KNIME AutoML , ML-Plan , Vertex AI .","title":"Fixed pipeline examples"},{"location":"gradient_descent_strategy_examples/","text":"","title":"Gradient descent strategy examples"},{"location":"graph_pipeline_examples/","text":"Auto-Pytorch , AutoGOAL , AutoKeras , TPOT , TransmogrifAI .","title":"Graph pipeline examples"},{"location":"grid_strategy_examples/","text":"H2O AutoML .","title":"Grid strategy examples"},{"location":"hill_climbing_strategy_examples/","text":"Hyperopt-sklearn , KNIME AutoML .","title":"Hill climbing strategy examples"},{"location":"hyperband_strategy_examples/","text":"","title":"Hyperband strategy examples"},{"location":"linear_pipeline_examples/","text":"AutoGOAL , RECIPE .","title":"Linear pipeline examples"},{"location":"monte_carlo_strategy_examples/","text":"ML-Plan .","title":"Monte carlo strategy examples"},{"location":"portfolio_meta_examples/","text":"Auto-Pytorch .","title":"Portfolio meta examples"},{"location":"random_strategy_examples/","text":"AutoGOAL , Azure ML , H2O AutoML , Hyperopt-sklearn , KNIME AutoML , TransmogrifAI , Vertex AI .","title":"Random strategy examples"},{"location":"reinforcement_learning_strategy_examples/","text":"","title":"Reinforcement learning strategy examples"},{"location":"search-spaces/","text":"Search spaces \u00b6 Warning This section is under construction In the context of AutoML, a search space is an implicitly- or explicitly-defined collection of machine learning pipelines, among which to search for a suitable solution to a given machine learning problem. The search space in any given AutoML system ultimately defines which solutions are possible at all. To characterize common search spaces, we'll focus first on the building blocks of any search space: operators , hyperparameters , and pipelines . Then we'll look at common characteristics that can help us analize and compare the search spaces in current AutoML systems. Operators \u00b6 An operator is any atomic component that performs a given function in a machine learning pipeline: e.g., a tokenization algorithm, a feature selector, or a classification model. For the purpose of AutoML, operators are often black-box; that is, we don't care about their internal structure, we just care about whether they can fit in any given pipeline. Operators are thus often characterized by input, output, and a set of hyperparameters . The input and output define how that operator interacts with other operators in any given pipeline. This is not as simple as defining an input and output type , since often types, at least in their conventional definition in programming languages (i.e., independent types), are insufficient to completely characterize whether an operator can act on a given input. For example, most algorithms in scikit-learn take matrices as input, but some can only act on dense matrices, while others work for both dense and sparse matrices. Similarly, in NAS, most operators are neural layers, which all receive tensors as inputs. However, they work on specific tensor shapes , and we cannot connect a layer \\(l_1\\) to another layer \\(l_2\\) unless their output and input shapes match, respectively. Therefore, to completely characterize an operator, an AutoML systems needs and implict or explicit typing system that is able to capture these restrictions. The more flexible the pipeline representation and the more varied the types of operators involved, the more sophisticated the typing system should be. In fixed-size pipelines , for example, it often suffices to consider that every operator in each step has a definite input and output type. However, linear or graph-based pipelines need explicit or implicit restrictions about which operators can connect to each other. Models \u00b6 One special type of operators are models, which have that have internal parameters which are adjusted from training data. For example, in a decision tree classifier, the structure of the tree is adjusted such that it maximizes the probability of classifying correctly all the elements in the training set. The difference between models and the other operators is important because every machine learning pipeline ultimately fits one or more models. All the remaining operators are there for secundary, even if often crucial, tasks, such as feature preprocessing or dimensionality reduction. Models break the operator as a black-box illusion in one important sense: they have two modes of operation that must be dealt with explicitely in the AutoML engine: training and prediction. Contrary to the other, model-free operators, models must be run once on training data to adjust their parameters, and only then can they be actually used on new data. Thus, all AutoML systems must somehow deal with this two-mode operation issue. The most common strategy is to consider all operators to work in these two modes, with model-free operators just ignoring whatever mode they're in. Hyperparameters \u00b6 Hyperparameters are the tunable values of any operator that are not adjusted from data, but must be decided with a data-independent strategy. Examples include the number of layers or the activation functions in a neural network, the regularization strength in a linear classifier, or the maximum depth in a tree-based model. In a sense, the whole purpose of AutoML can be defined as finding the optimal configuration for all the hyperparameters involved in a set of selected operators. In fact, some AutoML paradigms are built entirely on top of the hyperparameter optimzation conceptualization. Paradigms like CASH consider the selection of operators as categorical hyperparameters , defining an implicit two-level hierarchical space . Continuous and discrete hyperparameters \u00b6 The simplest types of hyperparameters are numerical values, either continuous or discrete. Examples include the regularization strength in logistic regression, or the maximum depth in a decision tree. In the simplest case, a numerical hyperparameter has a defined range of valid values for the optimizer can choose from. From a probabilistic point of view, this represents an implicit uniform distribution on the hyperparameter domain. However, in some cases it makes more sense to explore the domain a specific hyperparameter with a different distribution. As an example, take the regularization strength \\(R\\) of a simple logistic regression model. The sensible values for \\(R\\) may lie anywhere between \\(10^{-6}\\) to \\(1\\) . Asuming a uniform distribution in this case means that values between \\(0.1\\) and \\(1\\) will have roughly 90% of the attention of the optimizer, and values between \\(10^{-6}\\) and \\(10^{-5}\\) will be selected one in a million times. That's probably not what we want, but, on the contrary, we expect values between, say \\(10^{-3}\\) and \\(10^{-2}\\) to be as likely as those between \\(10{^-2}\\) and \\(10^{-1}\\) . In this case, we can define a logarithmic distribution instead of a uniform distribution, effectively stretching out the smallest scales of the hyperparameter range. Categorical hyperparameters \u00b6 Categorical hyperparameters are used to model qualitatively different choices, such as the kernel function in a support vector machine, the activation function in a neural network's layer, or the regularization penalty function in a logistic regression. Since categorical values have no inherent order, they should not be treated as discrete values. The most common strategy is to define a categorical distribution, which assigns a probability \\(p_i\\) to every value \\(h_i\\) of a given hyperparameter \\(h\\) , such that \\(\\sum p_i = 1\\) . Conditional hyperparameters \u00b6 Conditional hyperparameters appear when one or more hyperparameters only make sense sometimes, that is, conditioned to some other. The simplest case is in a two-level hierarchical space , where the first level defines the type of model, and the second level defines the hyperparameters of each model. For example, suppose we have three different operators to choose from: SVM, logistic regression, and decision trees, each with their own hyperparameters. Lets assume the selection of which operator to use is indicated by a categorical hyperparameter called model . The value of the hyperparameter kernel_function is only relevant when model=svm . The hyperparameter kernel_function is thus said to be conditioned to the value of the hyperparameter model . Conditional hyperparameters can also appear in non-hierarchical spaces. For example, suppose we have again an SVM operator with different kernel functions available: linear , rbf and poly . When kernel_function=poly , a new hyperparameter degree becomes relevant, which is not used in the other kernel functions. As long as dependencies among hyperparameters are acyclical, we can transform the search space into a hierarchical one, such that conditional hyperparameters are selected in a level below the hyperparameters they depend on. In some cases we have constraints that restrict two hyperparameters from simultaneously taking some pairs of values. These cases cannot be easily transformed into hierarchical spaces without introducing an arbitrary order between the hyperparameters. The most flexible AutoML systems have mostly conditioned hyperparameters, which in turn result in highly hierarchical search spaces. Pipelines \u00b6 According to how flexible these pipelines are, we can identify four basic types: Single model pipelines \u00b6 When a single model is trained end-to-end, which is often an estimator (e.g., a classifier or regressor). An example is training a linear regression model for price estimation on tabular data, or fine-tunning a concrete neural network architecture end-to-end for image classification. Examples: AutoGOAL , H2O AutoML . Fixed-size pipelines \u00b6 When a few fixed atomic steps are considered, e.g., data preprocessing, feature selection, dimensionality reduction, and classification. In each step, several different algorithms with they respective hyperparameters can be considered. Examples: Auto-Pytorch , Auto-Sklearn , Auto-WEKA , AutoGOAL , AutoGluon , AutoNLP , Azure ML , H2O AutoML , Hyperopt-sklearn , KNIME AutoML , ML-Plan , Vertex AI . Linear pipelines \u00b6 Examples: AutoGOAL , RECIPE . Graph-based pipelines \u00b6 Examples: Auto-Pytorch , AutoGOAL , AutoKeras , TPOT , TransmogrifAI . Other features of search spaces \u00b6 Hierarchical spaces \u00b6 Probabilistic spaces \u00b6 Differentiable spaces \u00b6 Implicit versus explicit spaces \u00b6","title":"Search Space"},{"location":"search-spaces/#search-spaces","text":"Warning This section is under construction In the context of AutoML, a search space is an implicitly- or explicitly-defined collection of machine learning pipelines, among which to search for a suitable solution to a given machine learning problem. The search space in any given AutoML system ultimately defines which solutions are possible at all. To characterize common search spaces, we'll focus first on the building blocks of any search space: operators , hyperparameters , and pipelines . Then we'll look at common characteristics that can help us analize and compare the search spaces in current AutoML systems.","title":"Search spaces"},{"location":"search-spaces/#operators","text":"An operator is any atomic component that performs a given function in a machine learning pipeline: e.g., a tokenization algorithm, a feature selector, or a classification model. For the purpose of AutoML, operators are often black-box; that is, we don't care about their internal structure, we just care about whether they can fit in any given pipeline. Operators are thus often characterized by input, output, and a set of hyperparameters . The input and output define how that operator interacts with other operators in any given pipeline. This is not as simple as defining an input and output type , since often types, at least in their conventional definition in programming languages (i.e., independent types), are insufficient to completely characterize whether an operator can act on a given input. For example, most algorithms in scikit-learn take matrices as input, but some can only act on dense matrices, while others work for both dense and sparse matrices. Similarly, in NAS, most operators are neural layers, which all receive tensors as inputs. However, they work on specific tensor shapes , and we cannot connect a layer \\(l_1\\) to another layer \\(l_2\\) unless their output and input shapes match, respectively. Therefore, to completely characterize an operator, an AutoML systems needs and implict or explicit typing system that is able to capture these restrictions. The more flexible the pipeline representation and the more varied the types of operators involved, the more sophisticated the typing system should be. In fixed-size pipelines , for example, it often suffices to consider that every operator in each step has a definite input and output type. However, linear or graph-based pipelines need explicit or implicit restrictions about which operators can connect to each other.","title":"Operators"},{"location":"search-spaces/#models","text":"One special type of operators are models, which have that have internal parameters which are adjusted from training data. For example, in a decision tree classifier, the structure of the tree is adjusted such that it maximizes the probability of classifying correctly all the elements in the training set. The difference between models and the other operators is important because every machine learning pipeline ultimately fits one or more models. All the remaining operators are there for secundary, even if often crucial, tasks, such as feature preprocessing or dimensionality reduction. Models break the operator as a black-box illusion in one important sense: they have two modes of operation that must be dealt with explicitely in the AutoML engine: training and prediction. Contrary to the other, model-free operators, models must be run once on training data to adjust their parameters, and only then can they be actually used on new data. Thus, all AutoML systems must somehow deal with this two-mode operation issue. The most common strategy is to consider all operators to work in these two modes, with model-free operators just ignoring whatever mode they're in.","title":"Models"},{"location":"search-spaces/#hyperparameters","text":"Hyperparameters are the tunable values of any operator that are not adjusted from data, but must be decided with a data-independent strategy. Examples include the number of layers or the activation functions in a neural network, the regularization strength in a linear classifier, or the maximum depth in a tree-based model. In a sense, the whole purpose of AutoML can be defined as finding the optimal configuration for all the hyperparameters involved in a set of selected operators. In fact, some AutoML paradigms are built entirely on top of the hyperparameter optimzation conceptualization. Paradigms like CASH consider the selection of operators as categorical hyperparameters , defining an implicit two-level hierarchical space .","title":"Hyperparameters"},{"location":"search-spaces/#continuous-and-discrete-hyperparameters","text":"The simplest types of hyperparameters are numerical values, either continuous or discrete. Examples include the regularization strength in logistic regression, or the maximum depth in a decision tree. In the simplest case, a numerical hyperparameter has a defined range of valid values for the optimizer can choose from. From a probabilistic point of view, this represents an implicit uniform distribution on the hyperparameter domain. However, in some cases it makes more sense to explore the domain a specific hyperparameter with a different distribution. As an example, take the regularization strength \\(R\\) of a simple logistic regression model. The sensible values for \\(R\\) may lie anywhere between \\(10^{-6}\\) to \\(1\\) . Asuming a uniform distribution in this case means that values between \\(0.1\\) and \\(1\\) will have roughly 90% of the attention of the optimizer, and values between \\(10^{-6}\\) and \\(10^{-5}\\) will be selected one in a million times. That's probably not what we want, but, on the contrary, we expect values between, say \\(10^{-3}\\) and \\(10^{-2}\\) to be as likely as those between \\(10{^-2}\\) and \\(10^{-1}\\) . In this case, we can define a logarithmic distribution instead of a uniform distribution, effectively stretching out the smallest scales of the hyperparameter range.","title":"Continuous and discrete hyperparameters"},{"location":"search-spaces/#categorical-hyperparameters","text":"Categorical hyperparameters are used to model qualitatively different choices, such as the kernel function in a support vector machine, the activation function in a neural network's layer, or the regularization penalty function in a logistic regression. Since categorical values have no inherent order, they should not be treated as discrete values. The most common strategy is to define a categorical distribution, which assigns a probability \\(p_i\\) to every value \\(h_i\\) of a given hyperparameter \\(h\\) , such that \\(\\sum p_i = 1\\) .","title":"Categorical hyperparameters"},{"location":"search-spaces/#conditional-hyperparameters","text":"Conditional hyperparameters appear when one or more hyperparameters only make sense sometimes, that is, conditioned to some other. The simplest case is in a two-level hierarchical space , where the first level defines the type of model, and the second level defines the hyperparameters of each model. For example, suppose we have three different operators to choose from: SVM, logistic regression, and decision trees, each with their own hyperparameters. Lets assume the selection of which operator to use is indicated by a categorical hyperparameter called model . The value of the hyperparameter kernel_function is only relevant when model=svm . The hyperparameter kernel_function is thus said to be conditioned to the value of the hyperparameter model . Conditional hyperparameters can also appear in non-hierarchical spaces. For example, suppose we have again an SVM operator with different kernel functions available: linear , rbf and poly . When kernel_function=poly , a new hyperparameter degree becomes relevant, which is not used in the other kernel functions. As long as dependencies among hyperparameters are acyclical, we can transform the search space into a hierarchical one, such that conditional hyperparameters are selected in a level below the hyperparameters they depend on. In some cases we have constraints that restrict two hyperparameters from simultaneously taking some pairs of values. These cases cannot be easily transformed into hierarchical spaces without introducing an arbitrary order between the hyperparameters. The most flexible AutoML systems have mostly conditioned hyperparameters, which in turn result in highly hierarchical search spaces.","title":"Conditional hyperparameters"},{"location":"search-spaces/#pipelines","text":"According to how flexible these pipelines are, we can identify four basic types:","title":"Pipelines"},{"location":"search-spaces/#single-model-pipelines","text":"When a single model is trained end-to-end, which is often an estimator (e.g., a classifier or regressor). An example is training a linear regression model for price estimation on tabular data, or fine-tunning a concrete neural network architecture end-to-end for image classification. Examples: AutoGOAL , H2O AutoML .","title":"Single model pipelines"},{"location":"search-spaces/#fixed-size-pipelines","text":"When a few fixed atomic steps are considered, e.g., data preprocessing, feature selection, dimensionality reduction, and classification. In each step, several different algorithms with they respective hyperparameters can be considered. Examples: Auto-Pytorch , Auto-Sklearn , Auto-WEKA , AutoGOAL , AutoGluon , AutoNLP , Azure ML , H2O AutoML , Hyperopt-sklearn , KNIME AutoML , ML-Plan , Vertex AI .","title":"Fixed-size pipelines"},{"location":"search-spaces/#linear-pipelines","text":"Examples: AutoGOAL , RECIPE .","title":"Linear pipelines"},{"location":"search-spaces/#graph-based-pipelines","text":"Examples: Auto-Pytorch , AutoGOAL , AutoKeras , TPOT , TransmogrifAI .","title":"Graph-based pipelines"},{"location":"search-spaces/#other-features-of-search-spaces","text":"","title":"Other features of search spaces"},{"location":"search-spaces/#hierarchical-spaces","text":"","title":"Hierarchical spaces"},{"location":"search-spaces/#probabilistic-spaces","text":"","title":"Probabilistic spaces"},{"location":"search-spaces/#differentiable-spaces","text":"","title":"Differentiable spaces"},{"location":"search-spaces/#implicit-versus-explicit-spaces","text":"","title":"Implicit versus explicit spaces"},{"location":"search-strategies/","text":"Search strategies \u00b6 Warning This section is under construction A search strategy is an algorithm that finds well-performing pipelines in a given search space . Most search strategies are applicable to a variety of search spaces, but often require some specific characteristics. For example, bayesian optimization requires probabilistic spaces, because it uses the probabilistic distribution as the means to explore the space. The other ingredient of a search strategy, once a search space is defined, is a performance metric to optimize. Most search strategies are compatible with multiple metrics, but some have pre-requisites as well. For example, gradient descent methods requires the performance metric to be differentiable with respect to the search space. Sampling-based methods \u00b6 Random search \u00b6 Examples: AutoGOAL , Azure ML , H2O AutoML , Hyperopt-sklearn , KNIME AutoML , TransmogrifAI , Vertex AI . Grid search \u00b6 Examples: H2O AutoML . Hill climbing \u00b6 Examples: Hyperopt-sklearn , KNIME AutoML . Gradient descent \u00b6 Examples: Evolutionary search \u00b6 Examples: AutoGOAL , RECIPE , TPOT . Bayesian optimization \u00b6 Examples: Auto-Pytorch , Auto-Sklearn , Auto-WEKA , AutoKeras , Azure ML , Hyperopt-sklearn , KNIME AutoML , Vertex AI . Constructive methods \u00b6 Monte Carlo tree search \u00b6 Examples: ML-Plan . Reinforced learning \u00b6 Examples: Meta-learning \u00b6 Portfolio-based methods \u00b6 Examples: Auto-Pytorch . Warm starting \u00b6 Examples: Auto-Sklearn .","title":"Search Strategies"},{"location":"search-strategies/#search-strategies","text":"Warning This section is under construction A search strategy is an algorithm that finds well-performing pipelines in a given search space . Most search strategies are applicable to a variety of search spaces, but often require some specific characteristics. For example, bayesian optimization requires probabilistic spaces, because it uses the probabilistic distribution as the means to explore the space. The other ingredient of a search strategy, once a search space is defined, is a performance metric to optimize. Most search strategies are compatible with multiple metrics, but some have pre-requisites as well. For example, gradient descent methods requires the performance metric to be differentiable with respect to the search space.","title":"Search strategies"},{"location":"search-strategies/#sampling-based-methods","text":"","title":"Sampling-based methods"},{"location":"search-strategies/#random-search","text":"Examples: AutoGOAL , Azure ML , H2O AutoML , Hyperopt-sklearn , KNIME AutoML , TransmogrifAI , Vertex AI .","title":"Random search"},{"location":"search-strategies/#grid-search","text":"Examples: H2O AutoML .","title":"Grid search"},{"location":"search-strategies/#hill-climbing","text":"Examples: Hyperopt-sklearn , KNIME AutoML .","title":"Hill climbing"},{"location":"search-strategies/#gradient-descent","text":"Examples:","title":"Gradient descent"},{"location":"search-strategies/#evolutionary-search","text":"Examples: AutoGOAL , RECIPE , TPOT .","title":"Evolutionary search"},{"location":"search-strategies/#bayesian-optimization","text":"Examples: Auto-Pytorch , Auto-Sklearn , Auto-WEKA , AutoKeras , Azure ML , Hyperopt-sklearn , KNIME AutoML , Vertex AI .","title":"Bayesian optimization"},{"location":"search-strategies/#constructive-methods","text":"","title":"Constructive methods"},{"location":"search-strategies/#monte-carlo-tree-search","text":"Examples: ML-Plan .","title":"Monte Carlo tree search"},{"location":"search-strategies/#reinforced-learning","text":"Examples:","title":"Reinforced learning"},{"location":"search-strategies/#meta-learning","text":"","title":"Meta-learning"},{"location":"search-strategies/#portfolio-based-methods","text":"Examples: Auto-Pytorch .","title":"Portfolio-based methods"},{"location":"search-strategies/#warm-starting","text":"Examples: Auto-Sklearn .","title":"Warm starting"},{"location":"single_pipeline_examples/","text":"AutoGOAL , H2O AutoML .","title":"Single pipeline examples"},{"location":"systems/","text":"A list of popular AutoML systems \u00b6 This section lists all the AutoML systems considered for the present survey and the associated metadata. A description of the characteristics considered for each system can be found in the Comparison section . Help We need your help! If you want to contribute to this section, you can either edit any of the systems' characteristics by clicking on each section's badge, or add new systems in our repository. AI Builder \u00b6 Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture Auto-Pytorch \u00b6 Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture Auto-Sklearn \u00b6 Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture Auto-WEKA \u00b6 Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture AutoGOAL \u00b6 Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture AutoGluon \u00b6 Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture AutoKeras \u00b6 AutoKeras focuses on neural architecture search, using bayesian optimization with a custom acquistion function based on network morphism. Presents a high-level API inspired by scikit-learn , where users only need to select a task-specific model; and a low-level API where users can completely customize a neural search space. Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture AutoNLP \u00b6 Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture Azure ML \u00b6 Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture H2O AutoML \u00b6 Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture Hyperopt-sklearn \u00b6 Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture KNIME AutoML \u00b6 Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture ML-Plan \u00b6 Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture RECIPE \u00b6 Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture TPOT \u00b6 Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture TransmogrifAI \u00b6 Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture Vertex AI \u00b6 Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture","title":"Systems"},{"location":"systems/#a-list-of-popular-automl-systems","text":"This section lists all the AutoML systems considered for the present survey and the associated metadata. A description of the characteristics considered for each system can be found in the Comparison section . Help We need your help! If you want to contribute to this section, you can either edit any of the systems' characteristics by clicking on each section's badge, or add new systems in our repository.","title":"A list of popular AutoML systems"},{"location":"systems/#ai-builder","text":"Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture","title":"AI Builder"},{"location":"systems/#auto-pytorch","text":"Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture","title":"Auto-Pytorch"},{"location":"systems/#auto-sklearn","text":"Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture","title":"Auto-Sklearn"},{"location":"systems/#auto-weka","text":"Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture","title":"Auto-WEKA"},{"location":"systems/#autogoal","text":"Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture","title":"AutoGOAL"},{"location":"systems/#autogluon","text":"Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture","title":"AutoGluon"},{"location":"systems/#autokeras","text":"AutoKeras focuses on neural architecture search, using bayesian optimization with a custom acquistion function based on network morphism. Presents a high-level API inspired by scikit-learn , where users only need to select a task-specific model; and a low-level API where users can completely customize a neural search space. Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture","title":"AutoKeras"},{"location":"systems/#autonlp","text":"Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture","title":"AutoNLP"},{"location":"systems/#azure-ml","text":"Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture","title":"Azure ML"},{"location":"systems/#h2o-automl","text":"Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture","title":"H2O AutoML"},{"location":"systems/#hyperopt-sklearn","text":"Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture","title":"Hyperopt-sklearn"},{"location":"systems/#knime-automl","text":"Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture","title":"KNIME AutoML"},{"location":"systems/#ml-plan","text":"Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture","title":"ML-Plan"},{"location":"systems/#recipe","text":"Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture","title":"RECIPE"},{"location":"systems/#tpot","text":"Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture","title":"TPOT"},{"location":"systems/#transmogrifai","text":"Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture","title":"TransmogrifAI"},{"location":"systems/#vertex-ai","text":"Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture","title":"Vertex AI"},{"location":"systems_list/","text":"AI Builder \u00b6 Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture Auto-Pytorch \u00b6 Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture Auto-Sklearn \u00b6 Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture Auto-WEKA \u00b6 Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture AutoGOAL \u00b6 Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture AutoGluon \u00b6 Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture AutoKeras \u00b6 AutoKeras focuses on neural architecture search, using bayesian optimization with a custom acquistion function based on network morphism. Presents a high-level API inspired by scikit-learn , where users only need to select a task-specific model; and a low-level API where users can completely customize a neural search space. Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture AutoNLP \u00b6 Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture Azure ML \u00b6 Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture H2O AutoML \u00b6 Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture Hyperopt-sklearn \u00b6 Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture KNIME AutoML \u00b6 Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture ML-Plan \u00b6 Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture RECIPE \u00b6 Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture TPOT \u00b6 Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture TransmogrifAI \u00b6 Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture Vertex AI \u00b6 Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture","title":"Systems list"},{"location":"systems_list/#ai-builder","text":"Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture","title":"AI Builder"},{"location":"systems_list/#auto-pytorch","text":"Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture","title":"Auto-Pytorch"},{"location":"systems_list/#auto-sklearn","text":"Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture","title":"Auto-Sklearn"},{"location":"systems_list/#auto-weka","text":"Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture","title":"Auto-WEKA"},{"location":"systems_list/#autogoal","text":"Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture","title":"AutoGOAL"},{"location":"systems_list/#autogluon","text":"Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture","title":"AutoGluon"},{"location":"systems_list/#autokeras","text":"AutoKeras focuses on neural architecture search, using bayesian optimization with a custom acquistion function based on network morphism. Presents a high-level API inspired by scikit-learn , where users only need to select a task-specific model; and a low-level API where users can completely customize a neural search space. Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture","title":"AutoKeras"},{"location":"systems_list/#autonlp","text":"Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture","title":"AutoNLP"},{"location":"systems_list/#azure-ml","text":"Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture","title":"Azure ML"},{"location":"systems_list/#h2o-automl","text":"Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture","title":"H2O AutoML"},{"location":"systems_list/#hyperopt-sklearn","text":"Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture","title":"Hyperopt-sklearn"},{"location":"systems_list/#knime-automl","text":"Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture","title":"KNIME AutoML"},{"location":"systems_list/#ml-plan","text":"Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture","title":"ML-Plan"},{"location":"systems_list/#recipe","text":"Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture","title":"RECIPE"},{"location":"systems_list/#tpot","text":"Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture","title":"TPOT"},{"location":"systems_list/#transmogrifai","text":"Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture","title":"TransmogrifAI"},{"location":"systems_list/#vertex-ai","text":"Basic info Interfaces Domains Techniques Tasks Search strategies Search space Architecture","title":"Vertex AI"},{"location":"warm_start_meta_examples/","text":"Auto-Sklearn .","title":"Warm start meta examples"}]}